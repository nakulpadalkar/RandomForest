\documentclass[11pt,aspectratio=1610,usenames,dvipsnames]{beamer} % Beamer size: 1610, 149, 54, 43 and 32.
\usetheme{CambridgeUS} % Ilmenau Frankfurt Dresden Warsaw Szeged Boadilla fibeamer
\usecolortheme{beaver}
\usepackage{minted}
\usemintedstyle{tango}
%\input{../nakuldesign.tex}
\input{../minimaldesign.tex}
\usepackage[]{algorithm2e}
\pgfplotsset{compat=1.7}
%\usepackage{enumitem}
%\newcounter{descriptcount}
%\newlist{enumdescript}{description}{1}
%\setlist[enumdescript,1]{%
%	before={\setcounter{descriptcount}{0}%
%		\renewcommand*\thedescriptcount{\alph{descriptcount}}}
%	,font=\bfseries\stepcounter{descriptcount}\thedescriptcount~
%}
%\setlist[enumdescript,2]{%
%	before={\setcounter{descriptcount}{0}%
%		\renewcommand*\thedescriptcount{\roman{descriptcount}}}
%	,font=\bfseries\stepcounter{descriptcount}\thedescriptcount~
%}

\definecolor{uured}{rgb}{0.6,0,0}
\definecolor{uulgr}{rgb}{0.7,0.7,0.7}
\definecolor{uullgr}{rgb}{0.9,0.9,0.9}


\title{Tree Ensemble Methods}
\subtitle{Decision Trees}

\author[Nakul R Padalkar] % (optional, for multiple authors)
{\bfseries Nakul R Padalkar} %\and J.~Doe\inst{2}}

\institute[Georgetown University]{\MakeUppercase{Georgetown University}} % (optional)
\date{\it\today}

\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}




\begin{document}


\begin{frame}[plain]
\maketitle
\end{frame}

\begin{frame}{Overview}

\begin{multicols}{2}
	\tableofcontents[]
\end{multicols}

\end{frame}


\section{Ensemble methods}
\begin{frame}{Reference}
	Most of this presentation is based on the Microsoft Research technical report TR-2011-114.
\end{frame}

\begin{frame}{General idea of ensembles}
\begin{itemize}
	\item 	The idea of an ensemble is simple: If it difficult to find one really good model perhaps we can find several weaker models and combine their predictions.
	\item A simple example: Say you have one outcome $Y$ and 4 covariates $ X_1, X_2, X_3, X_4 $. The goal is to predict Y. A possible ensemble would be to fit
\end{itemize}
	\begin{align*}
		y=\alpha_1 + \beta_1 X_1 + \epsilon_1\\
		y=\alpha_2 + \beta_2 X_2 + \epsilon_2\\
		y=\alpha_3 + \beta_3 X_3 + \epsilon_3\\
		y=\alpha_4 + \beta_4 X_4 + \epsilon_4
	\end{align*}%
\begin{itemize}
	\item 	and then use the mean of their predictions
\end{itemize}
	\begin{equation}
		\hat{y}_{ensemble}=\frac{1}{4}\sum_{i=1}^{4} \hat{y}=\frac{1}{4}\sum_{i=1}^{4}(\hat{\alpha}_i + \hat{\beta}_iX_i)
	\end{equation}
	
\end{frame}

%

\begin{frame}{Two key parts of an ensemble}
	\begin{itemize}
		\item The prediction models (sometimes called `learners' in ML literature)
		\begin{itemize}
			\item A single model in an ensemble can be a simple or a complex model
			\item Often the ensemble contains many simple models.
			\item Wikipedia:``\textit{In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone}''
		\end{itemize}
		\item The weighting of each prediction in the final ensemble prediction
		\begin{itemize}
			\item Models/learners with better predictive power can be given larger weights in the final prediction
			\item There are many complex algorithms for weighting together the predictions from many models
		\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}{Ensembles of decision trees}
	The most common type of ensembles is ensembles of decision trees.
	
	We will focus on this case, but note that any type of model can be included in an ensemble in principle.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\section{Bagging and Boosting}
\begin{frame}{Bagging and Boosting}
	
	Remember, the error of a prediction/classification can be decomposed as
	\begin{align*}
		error = bias + variance + \text{bayes error}.
	\end{align*}
	\begin{itemize}
		\item Complex models/strong learners (with many parameters) tend to have small bias and large variance (tend to be overfitted)
		\item Shallow models/weak learners (with few parameters) tend to have small variance and large bias
		\item \textbf{Bagging:} Ensemble methods that aim to decrease the variance of complex/strong learners with low bias and large variance
		\item \textbf{Boosting:} Ensemble methods that aim to decrease the bias of shallow/weak learners with low variance and large bias
	\end{itemize}
	
	%That is: Either choose models with small bias and reduce the variance with bagging, or choose models with small variance and reduce bias with boosting
\end{frame}


\section{Bagging}

\begin{frame}{Bagging}
	Consider a sample of $N$ units.
	\textbf{Bagging algorithm:}
	\begin{enumerate}
		\item Draw, \textit{with replacement}, a random sample of $N$ units from the original sample
		\item Fit a prediction model (e.g., a decision tree)
		\item Repeat steps 1-2 $B$ times
		\item Weight together the predictions from the B models into a final ensemble prediction
	\end{enumerate}
	Train several deep trees and combine their results by weighting together their predictions
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Random forests}

\begin{frame}{Random Forest}
	
	A random forest is a bagging ensemble method, but with one extra step. Consider a sample of N units and K observed covariates/features
	
	Random forest algorithm:
	\begin{enumerate}
		\item Draw, \textit{with replacement}, a random sample of $N$ units from the original sample
		\item \textbf{Draw,  \textit{without replacement}, a random subset of $k$ covariates/features}
		\item Fit a prediction model (e.g., a decision tree)
		\item Repeat step 1-3 $B$ times
		\item Weight together the predictions from the B models into a final ensemble prediction
	\end{enumerate}
	
	It is common to use $k=\sqrt{K}$ (rounded down) for classification and $k=K/3$ for regression. But these are only rules of thumb: $k$ is a \textit{tuning parameter}.
\end{frame}

\begin{frame}{Random Forest, variance reduction}
	Consider each tree to be an i.i.d. random variable with variance $\sigma^2$.\\
	The average of these trees then have variance $\frac{1}{B}\sigma^2$. Trees constructed from the same set of covariates will be correlated and therefore not independent. The variance of the average of these correlated trees then becomes
	\begin{align*}
		\rho\sigma^2+\dfrac{1-\rho}{B}\sigma^2.
	\end{align*}
	The second term will vanish with increasing $B$ leaving just the first term left which is a function of the correlation between the trees and the variance. The remaining part of the variance is minimized by only consider a subset of the covariates when constructing trees - reducing the correlation between them.
\end{frame}


\begin{frame}{Differences between bagging and random forest:}
	\begin{itemize}
		\item In bagging, the trees are often highly correlated
		\begin{itemize}
			\item If some covariates are strong predictors of the outcome (in the training data), many trees in the `bag' will us the same covariates in their decisions
		\end{itemize}
		\item In a random forest, the trees are less similar/correlated since all covariates are not available when each tree is constructed.
	\end{itemize}
	This means that a random forest (with many trees) uses the predictive ability of all covariates rather than just a few, which usually improves out of sample performance.
	
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Boosting}

\begin{frame}{Boosting}
	In boosting, models are trained sequentially where each new model tries to target weak spots of the previous models in the ensemble to improve the performance of the ensemble
	
	Boosting
	\begin{enumerate}
		
		\item Fit a prediction model/classifier (e.g., a decision tree) using the original sample
		\begin{itemize}
			\item Give the misclassified observations higher weights
		\end{itemize}
		\item Draw, \textit{with replacement},\textit{with probability proportional to the weights}, a random sample of $N$ units from the original sample
		\item Fit a prediction model/classifier (e.g., a \emph{shallow} decision tree) using the new sample
		\item Update the weights of each observation according to the average misclassification of the trained classifiers
		\item Repeat step 2-4 B times
		\item Weight together the predictions from the B models into a final ensemble prediction, giving larger weights to classifiers with smaller errors
	\end{enumerate}
	
\end{frame}

\begin{frame}{Boosting}
	For boosting to work well, the updates of the weights must be chosen in some clever way. One successful method is \textit{gradient descent}.
	
	We will not focus more on the particular algorithms. For now, we are satisfied with understanding the concept of boosting:
	Train a bunch of classifiers sequentially. Force each new classifier to train more on data that the previous classifiers had problems with classifying by giving those samples a higher probability to be sampled.
\end{frame}


\begin{frame}{XGBoost}
	
	\begin{enumerate}

		\item State of the Art method
		\item Use gradient boosting trees with regularization
		\item Is scalable to very large data
	\end{enumerate}
\end{frame}


\section{Random Decision Forest}
\begin{frame}{Random Decision Forest}
	Most of the decision making problems can be automatized through machine learning algorithms. These can be generally divided in following categories.\\
	\begin{itemize}
		\item Categorization or classification problem.
		\item Regression problem.
		\item Density Estimation problem.
	\end{itemize}
\end{frame}
\begin{frame}{Examples}
\begin{itemize}
	\item Recognizing the category of a scene captured in a photograph.
	\item Predicting the price of a house as a function of its distance
	from a good school.
	\item Detecting abnormalities in a medical scan by using a learned probability density
	function for scans of healthy individuals.
	\item Capturing the intrinsic variability of size and shape of patients brains in MRI
	\item Interactive image segmentation.
	\item Learning a general rule for detecting tumors in images using minimal amount of manual annotations.
\end{itemize}
\end{frame}

\subsection{Background and notation}
\begin{frame}{Decision trees: basic idea}
	\begin{itemize}
		\item A popular method that can be used for both classification and regression is {\color{uured}decision trees}.
		\item We will discuss classification trees in this lecture - regression trees are analogous.
		\item Have you ever played the game ``20 questions''?
		\item Decision trees is more or less that game!
		\item In the case of classification, the idea is to classify the new observation by asking a series of questions. Depending on what the answer to the first question is, different second questions are asked, and so on. Questions are asked until a conclusion is reached.
	\end{itemize}
\end{frame}

\begin{frame}{Decision trees: basic idea}{Classification}
	
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\begin{enumerate}
				\item<1-> Consider Iris Flower Dataset
				\item<1-> sepal length =6.10 cm; sepal width =2.60 cm; petal length=5.60 cm; petal width 1.40 cm
				\item<1-> What is the petal length? $ PL \geq 2.45 $
				\item<2-> What is the petal width? $ PW \leq < 1.75$
				\item<3-> What is the petal length? $ PL \geq 4.95 $
				\item<4-> What is the petal width? $ PL \leq 1.55 $
			\end{enumerate}
		\end{column}
		~
		\begin{column}{0.55\textwidth}
			
			\includegraphics<1>[scale=0.60]{figures/ClassificationTree-Iris - 1.pdf}
			\includegraphics<2>[scale=0.60]{figures/ClassificationTree-Iris - 2.pdf}
			\includegraphics<3>[scale=0.55]{figures/ClassificationTree-Iris - 3.pdf}
			\includegraphics<4>[scale=0.50]{figures/ClassificationTree-Iris - 4.pdf}
%			\includegraphics<5>[scale=0.60]{figures/ClassificationTree-Iris - 5.pdf}			
		\end{column}
		
	\end{columns}
\end{frame}


\begin{frame}{Decision trees: basic idea}{Regression}
	
	\begin{columns}
		\begin{column}{0.50\textwidth}
			\begin{enumerate}
				\item<1-> Consider Boston Housing Prices Dataset
%				\item<1-> CRIM=1.4; ZN=0; INDUS=19.58; CHAS0=; NOX=0.87; RM=6.51; AGE=100;
%				\item<1-> DIS=1.77; RAD=5; TAX=403; PTRATIO=14.70; B=364.31; LSTAT=7.39;
				\item<2-> Average number of rooms per house? $ RM \leq 6.94 $
				\item<3-> Weighted distances to five Boston employment centers? $ 1.38 \geq DIS $
				\item<4-> \% of population has lower status? $ LSTAT < 14.4 $
				\item<5-> Else
			\end{enumerate}
		\end{column}
		~
		\begin{column}{0.50\textwidth}
			
			\includegraphics<1>[scale=0.60]{figures/RegressionTree-Boston - 1.pdf}
			\includegraphics<2>[scale=0.60]{figures/RegressionTree-Boston - 2.pdf}
			\includegraphics<3>[scale=0.50]{figures/RegressionTree-Boston - 3.pdf}
			\includegraphics<4-5>[scale=0.40]{figures/RegressionTree-Boston - 4.pdf}
			%		\includegraphics<3-4>[scale=0.45]{figures/RegressionTree-Boston - 2.pdf}
			
		\end{column}
		
	\end{columns}
\end{frame}


\begin{frame}{Classification Trees}
		\includegraphics<1>[scale = 0.40]{figures/Iris-SVM-decisionboundries.pdf}
		\includegraphics<2>[scale = 0.40]{figures/Iris-Logistic Regression-decisionboundries.pdf}
		\includegraphics<3>[scale = 0.40]{figures/Iris-Random Forest-decisionboundries.pdf}
		\includegraphics<4>[scale = 0.40]{figures/Iris-Naive Bayes-decisionboundries.pdf}
%		\includegraphics[scale = 0.65]{figures/Iris-SVM-decisionboundries.pdf}
				
\end{frame}


\begin{frame}{Regression Trees}
	
	\begin{align*}
		T(x) = \sum^M_{m=1} \gamma_m I(x \in R_m)\,,
	\end{align*}
	where $M$ is the total number of regions and $I(x \in R_m)$ is an indicator variable if $x_i$ belongs to region $R_m$. $\gamma_m$ is the prediction for region $m$.
	
\end{frame}


\begin{frame}{Growing a Decision Tree}
	
	\begin{enumerate}
		\item A tree has two groups of parameters $\Theta = (\gamma, R)$ that we need to learn.
		\item We want a tree that minimize $L(\theta) = (y_i - T_\Theta(x_i))^2$
		\item Usually we estimate $\gamma$ as the mean of $y_i$ in the region as:
		\[
		\hat{\gamma}_m = \frac{1}{N_m} \sum_{x_i \in R_m}^{N_m} y_i\,,
		\]
		where $N_m$ is the number of observations in region $R_m$.
		\item Learning $R_m$ is generally computationally infeasable so we use a greedy heuristic.
	\end{enumerate}
	
\end{frame}



\begin{frame}{Growing a Decision Tree: Greedy Algorithm}
	
	
	Let $athcal{S}$ be the set of all observations $\{1,...,N\}$ and \texttt{S[[m]]} be the set of observation indecies in $R_m$ and \texttt{l} is the minimal number of leafs per node.
	
	Input: $athcal{S}, \texttt{X}, \texttt{y}, \texttt{l}$
	
	\begin{enumerate}
		\item[] \texttt{S[[1]]} = $athcal{S}$,\texttt{M = 1}, \texttt{m = 1}
		\item[] \texttt{while m <= M} then do:
		\begin{enumerate}
			\item[] \texttt{if(size(S[[m]]) >= 2*l)}
			\begin{enumerate}
				\item[] \texttt{S[[M+1]]}, \texttt{S[[M+2]]}, \texttt{j[m]}, \texttt{s[m]} = \\ \texttt{split\_tree(X[S[[m]],], y[S[[m]],],l)}
				\item[] \texttt{M = M + 2}
			\end{enumerate}
			\item[] \texttt{else}
			\begin{enumerate}
				\item[] compute $\hat{\gamma}$ for \texttt{S[[m]]}
			\end{enumerate}
			\item[] \texttt{m = m + 1}
		\end{enumerate}
	\end{enumerate}
	
	Output: $\texttt{j}, \texttt{s}, \gamma$
	
	Example:
	\texttt{j = \{Years, Hits\}}, \texttt{s = \{4.5, 117.5\}}, $\hat{\gamma} = \{122, 317, 245\}$
	
\end{frame}

\begin{frame}{How to do a split?}
	
	Here we try to compute Eq. (9.13) in ESL:
	
	Input: $athbf{X}, athbf{y}, l$
	
	\begin{enumerate}
		\item[] $SS$ = Inf \# Sum of Squares
		\item[] for $j \in \{1,...,P\}$
		\begin{enumerate}
			\item[] for $k \in \{1,...,N\}$
			\begin{enumerate}
				\item[] $s$ = $X_{k,j}$ \# Split point
				\item[] if ($|R_1(s, j)| < l$ or $|R_2(s, j)| < l$) next \# Dont create too few leaves
				\item[] $c_1$ = $\frac{1}{|R_1(s, j)|} \sum_{x_i \in R_1(s, j)} y_i$
				\item[] $c_2$ = $\frac{1}{|R_2(s, j)|} \sum_{x_i \in R_2(s, j)} y_i$
				\item[] $SS_{i,j}$ = $\sum_{x_i \in R_1(s, j)} (y_i - c_1)^2 + \sum_{x_i \in R_2(s, j)} (y_i - c_2)^2$ \# Compute Sum of Squares
				\item[] $S_{i,j}$ = s
			\end{enumerate}
		\end{enumerate}
		\item[] $i_{final}, j_{final} = in_{i,j} SS$
		\item[] $s_{final} = S_{i_{final},j_{final}}$
		\item[] return $R_1(s_{final}, j_{final}), R_2(s_{final}, j_{final}), s_{final}, j_{final}$
	\end{enumerate}
\end{frame}



\begin{frame}{Decision trees: Classification Trees}
	
\begin{itemize}
	\item How do we do if we have a classification tree?
	\item We just change the loss function.
	\item Let $athbb{P}(j|t)$ be the fraction of observations in class $j$ at the node $t$ and let $c$ be the number of classes.
	\item The {\color{uured}Gini} for node $t$ is defined as
\begin{align*}
	Gini(t)=1-\sum_{j=1}^{c}(athbb{P}(j|t))^2
\end{align*}
	\item  Gini is a measure of ''impurity''. If all observations belong to the same class, then
\begin{align}
		Gini(t)=1-1^2-0-\ldots-0=0.
\end{align}
	\item The Gini is maximized when all classes have the same number of observations at $t$.
	\item One criterion for splitting could be to minimize the Gini in the next level of the tree. That way we will get ''purer'' nodes.
\end{itemize}
\end{frame}


\begin{frame}{Decision trees: the best split}
	The situation becomes a bit more complicated if we take into account that the children can have different numbers of observations.  To account for this, we try to maximize the gain:
\begin{align*}
	Gain=Gini(t)-\sum_{j=1}^{k} \frac{n_{v_j}}{n_t}Gini(v_j)
\end{align*}
	where $v_j$ are the children and $n_i$ are the number of observations at node $i$.
	This is equivalent to minimizing $\sum_{j=1}^k \frac{n_{v_j}}{n_t}Gini(v_j).$
	Sometimes other impurity measures than Gini are used. One example is the {\color{uured}entropy}:
\begin{align*}
		Entropy(t)=-\sum_{i=1}^{c}athbb{P}(i|t)\log_{2}athbb{P}(i|t).
\end{align*}
\end{frame}


\begin{frame}{Decision trees: the best split}
%		\includegraphics<3>[scale = 0.40]{figures/Iris-Random Forest-decisionboundries.pdf}
	\begin{itemize}
		\item If we split the data horizontally it produces three sets of data.
		\item Each set is associated with a lower entropy (higher information, peakier histograms).
		\item The gain of information achieved by splitting the data is computed as
	\end{itemize}
	\begin{align*}
		H(s)&=-\sum_{i=1}^{C}athbb{P}(i|t)\log_{2}athbb{P}(i|t)\\
		I&=H(S)-\sum_{j=1}^{C} \frac{|S^{1}|}{|S|}H(S^{j})
	\end{align*}
%	where $v_j$ are the children and $n_i$ are the number of observations at node $i$.
%	This is equivalent to minimizing $\sum_{j=1}^k \frac{n_{v_j}}{n_t}Gini(v_j).$
%	Sometimes other impurity measures than Gini are used. One example is the {\color{uured}entropy}:
%	\begin{align*}
%		Entropy(t)=-\sum_{i=1}^{c}athbb{P}(i|t)\log_{2}athbb{P}(i|t).
%	\end{align*}
\end{frame}

\begin{frame}{Important concepts}
	\begin{itemize}
		\item {\color{uured}Tree depth:} the length of the longest path from the root to a leaf (i.e. greatest number of questions that the tree can ask).
		\item {\color{uured}Leaf size:} the number of observations in a leaf.
	\end{itemize}
	Decision trees can become quite large, which may lead to:
	\begin{itemize}
		\item Overfitting (high variance)
		\item Difficulties interpreting the tree
	\end{itemize}
	The solution to this is
	\begin{itemize}
		\item {\color{uured}Pruning:} forcing the tree to be smaller by adding a {\color{uured}stopping condition}, e.g. a maximum depth or minimal leaf size.
		\item But decision trees are quite bad predication models...
	\end{itemize}
\end{frame}

\subsection{Decision Forest}%
\begin{frame}{Decision Forests}%
	A random decision forest is an ensemble of randomly trained decision trees. For a decision forest to be used as a ensemble learners, a few criteria needs to be satisfied.
	\begin{itemize}%
		\item A family of split functions ("weak learners")
		\item A type of leaf predictor.
		\item The randomness (for selecting parameters)
	\end{itemize}%
\end{frame}%


\subsubsection{The weak learner model}
\begin{frame}{Weak Learner Model}

	\includegraphics[scale = 0.90]{figures/decisionForests-5.pdf}
	a binary split function: $ h(v, \theta_{j}) \in \{0,1\} $
	\begin{itemize}
		\item Parameters $ \theta = (\phi;\psi ;\tau) $
		\item $ \phi $ is feature subset
		\item $ \psi $ is a geometric separator (hyperplane)
		\item $ \tau $ is a threshold
	\end{itemize}
\end{frame}

%\begin{frame}{Linear Separator}
%\begin{columns}
%
%	\begin{column}{0.50\textwidth}
%	A classifier is linear if its decision boundary on the feature space is a linear function: positive and negative examples are separated by an hyperplane.
%		\begin{align*}
%		h(v, \theta_{j}) = \big[\tau_{1} > \phi(v)\cdot\psi> \tau_{2}\big]
%		\end{align*}
%	\begin{itemize}
%	\item each hyperplane is a stump (one root 2 leaf decision tree, boosting)
%	\item $ \phi $ is feature subset
%	\item $ \psi $ is a geometric separator (hyperplane)
%	\item $ \tau $ is a threshold
%	\end{itemize}		
%	\end{column}
%	~
%	\begin{column}{0.49\textwidth}
%	
%		\includegraphics<1>[scale = 0.32]{figures/Iris-Logistic Regression-decisionboundries.pdf}
%		\includegraphics<2>[scale = 0.32]{figures/Iris-Random Forest-decisionboundries.pdf}
%	
%	\end{column}
%	
%\end{columns}
%
%\end{frame}
%
% \begin{frame}{Non Linear Separator}
%\includegraphics<1>[scale = 0.40]{figures/Iris-SVM-decisionboundries.pdf}
%\includegraphics<2>[scale = 0.40]{figures/Iris-Naive 
%% \begin{columns}
%% 	\begin{column}{0.50\textwidth}
%%		
%% 	\end{column}
%% 	~
%% 	\begin{column}{0.50\textwidth}
%%		
%% 	\includegraphics<1>[scale = 0.40]{figures/Iris-SVM-decisionboundries.pdf}
%% 	\includegraphics<2>[scale = 0.40]{figures/Iris-Naive Bayes-decisionboundries.pdf}
%%		
%% 	\end{column}
%%	
%% \end{columns}
% \end{frame}

\begin{frame}{Generalization of RF}%
	\includegraphics[scale = 0.90]{figures/decisionForests-5.pdf}
%	a binary split function: $ h(v, \theta_{j}) \in \{0,1\} $
	\begin{itemize}%
		\item Versatile, can be used even in high dimensional data
		\item $ \phi_{j} $ represents a bootstrapped dataset (both, features and instances)
		\item The number of degrees of freedom of the weak learner influences generalization properties.
	\end{itemize}%
	\begin{align*}%
	\theta_{j}^{\ast} &= \arg \underset{\theta_{j}}{\max} \,I_{j} \\
	\text{ where } I_{j}&=\left(S_{j},S_{j}^{L},S_{j}^{R},\theta_{j}\right)
\end{align*}%
\end{frame}%


\section{Classification forests}%
\subsection{Classification algorithms in the literature}%
\subsection{Specializing the decision forest model for classification}%
\subsection{Effect of model parameters}%
\subsection{Maximum-margin properties}%
\subsection{Comparisons with alternative algorithms}%
\subsection{Human body tracking in Microsoft Kinect for XBox}%

\section{Regression forests}%
\subsection{Nonlinear regression in the literature}%
\subsection{Specializing the decision forest model for regression}%
\subsection{Effect of model parameters}%
\subsection{Comparison with alternative algorithms}%
\subsection{Semantic parsing of 3D computed tomography scans}%
\end{document}%
